Benchmark Name,Primary Focus,Description,Paper URL,Project/GitHub/Dataset URL
HELM Safety,"Holistic (Toxicity, Bias, Fairness)","A foundational framework from Stanford's CRFM for comprehensive model evaluation across scenarios like accuracy, robustness, fairness, bias, and toxicity.",http://arxiv.org/pdf/2211.09110,https://github.com/stanford-crfm/helm
AIR-Bench 2024,Regulatory & Policy Alignment,"A benchmark from Stanford's CRFM with 5,694 prompts aligned with government regulations and corporate policies across 314 risk categories.",https://arxiv.org/pdf/2407.17436v2,https://github.com/stanford-crfm/air-bench-2024
TrustLLM,Holistic (Multi-aspect Trustworthiness),"A large-scale benchmark evaluating LLM trustworthiness across six dimensions: Truthfulness, Safety, Fairness, Robustness, Privacy, and Machine Ethics, using over 30 datasets.",https://arxiv.org/abs/2401.05561,https://github.com/HowieHwong/TrustLLM
MLCommons AlLuminate,Standardized Risk Assessment (Hazards),"A benchmark suite from MLCommons with 24,000+ prompts across 12 hazard categories to produce safety grades for chat systems.",https://arxiv.org/abs/2404.12241,https://github.com/mlcommons/ailuminate
MLCommons Al Safety v0.5,Standardized Risk Assessment (Hazards),"A proof-of-concept benchmark from MLCommons with 43,090 templated test items across 7 hazard categories, accessible via the ModelBench tool.",https://arxiv.org/abs/2404.12241,https://github.com/mlcommons/modelbench
SafetyBench,Comprehensive Safety (Multiple Choice),"A benchmark from Tsinghua University with 11,435 multiple-choice questions in Chinese and English covering 7 safety categories.",https://arxiv.org/pdf/2309.07045,https://github.com/thu-coai/SafetyBench
AdvBench,"Adversarial Attacks, Jailbreaking","A benchmark for evaluating model robustness against adversarial attacks, using harmful strings and behaviors.",https://arxiv.org/abs/2307.15043,https://github.com/adversarial-benchmark/advbench
HarmBench,"Automated Red Teaming, Jailbreaking",A standardized evaluation framework for automated red teaming and measuring robust refusal against harmful behaviors.,https://www.microsoft.com/en-us/research/publication/harmbench-a-standardized-evaluation-framework-for-automated-red-teaming-and-robust-refusal/,https://github.com/centerforaisafety/HarmBench
BackdoorLLM,"Backdoor Attacks, Model Poisoning",A comprehensive benchmark for evaluating backdoor attacks and defenses in LLMs across various tasks like classification and generation.,https://arxiv.org/pdf/2408.12798,https://github.com/bboylyg/BackdoorLLM
JailBreakV,Multimodal Jailbreaking,"A large-scale benchmark with 28,000 text-image pairs to assess the robustness of Multimodal Large Language Models against jailbreak attacks.",https://arxiv.org/pdf/2404.03027,https://github.com/EddyLuo1232/JailBreakV_28K
TruthfulQA,"Truthfulness, Factual Consistency",A benchmark with 817 questions designed to measure a model's tendency to generate answers that mimic common human falsehoods.,https://arxiv.org/abs/2109.07958,https://github.com/sylinrl/TruthfulQA
ToxiGen,"Implicit Toxicity, Hate Speech","A large-scale, machine-generated dataset of 274,186 toxic and benign statements about 13 minority groups to detect implicit toxicity.",https://aclanthology.org/2022.acl-long.234/,https://github.com/microsoft/ToxiGen
Real Toxicity Prompts,Toxic Content Generation,"A dataset of 100,000 non-toxic sentence prompts from the web that are likely to elicit toxic continuations from language models.",https://arxiv.org/abs/2009.11462,https://github.com/allenai/real-toxicity-prompts
BBQ (Bias Benchmark for QA),Bias in Question Answering,A benchmark with template-based multiple-choice questions to evaluate social biases in question-answering models.,https://aclanthology.org/2022.findings-acl.165/,https://github.com/nyu-mll/bbq
HHH (Helpfulness Honesty Harmlessness),"Helpfulness, Honesty, Harmlessness","A human-preference dataset from Anthropic for evaluating and aligning models on helpfulness, honesty, and harmlessness.",https://arxiv.org/abs/2204.05862,https://github.com/anthropics/hh-rlhf
MultiTrust,Multimodal Trustworthiness,A comprehensive benchmark from Tsinghua University for evaluating MLLM trustworthiness across 32 tasks and 5 dimensions.,https://arxiv.org/pdf/2406.07057,https://github.com/thu-ml/MMTrustEval
Hallusion Bench,Visual Hallucination & Illusion,"A benchmark with 346 images and 1,129 questions to evaluate language hallucination and visual illusion in Vision-Language Models.",https://arxiv.org/pdf/2310.14566,https://github.com/tianyi-lab/HallusionBench
WalledEval,"Toolkit (Multilingual, Exaggerated Safety)","A toolkit hosting 35+ safety benchmarks, including tests for multilingual safety, prompt injections, and exaggerated safety.",https://arxiv.org/abs/2408.03837,https://github.com/walledai/walledeval
AlSafetyLab,"Framework (Attack, Defense, Evaluation)",A comprehensive and extensible framework from Tsinghua University for implementing and testing various AI safety mechanisms.,https://arxiv.org/pdf/2502.16776,https://github.com/thu-coai/AlSafetyLab
COMPL-AI Framework,Framework (EU AI Act Compliance),An open-source benchmarking suite that translates the EU AI Act into measurable technical requirements for LLMs.,https://arxiv.org/pdf/2410.07959,https://github.com/compl-ai/compl-ai
SimpleSafetyTests,"Self-harm, Violence, CSEA","100 manually-labeled unsafe prompts covering self-harm, violence, and Child Sexual Exploitation and Abuse.",https://arxiv.org/abs/2311.08370,https://github.com/stanford-crfm/simple-safety-tests
WildJailbreak,"Prompt-injection, Jailbreaking","A public, open-source repository collecting real-world prompt-injection attacks and jailbreak cases.",https://arxiv.org/abs/2406.18510,https://github.com/allenai/wildteaming
Aya Red-Teaming,Multilingual Toxicity & Safety,A multilingual red-teaming dataset for toxicity and cultural safety across 8 languages.,https://arxiv.org/abs/2402.06619,https://huggingface.co/datasets/CohereLabs/aya_redteaming
Virology Capabilities Test (VCT),Biosecurity Information Misuse,322 question-and-answer prompts on virology protocols to test for potential misuse of information.,https://arxiv.org/abs/2504.16137,https://github.com/stanford-crfm/virology-test
WMDP,WMD Proliferation Knowledge,A multiple-choice question dataset to test an LLM's knowledge related to the proliferation of CBRN weapons.,https://arxiv.org/abs/2403.03218,https://github.com/centerforaisafety/wmdp
DoNotAnswer,Extreme Unethical Requests,"A dataset of instructions that responsible language models should refuse, focusing on extreme illegal or unethical requests.",https://arxiv.org/abs/2401.05372,https://github.com/jayelm/do-not-answer
BOLD,Bias in Open-ended Language Generation,"A large-scale dataset with 23,679 prompts to evaluate social bias in open-ended text generation across five domains.",https://arxiv.org/abs/2101.05783,https://github.com/amazon-science/bold
BAD (Bias Amplification Dataset),Bias Amplification,A dataset designed to probe whether and how much language models amplify societal biases present in their training data.,https://arxiv.org/abs/2109.00036,https://github.com/stanfordnlp/BAD
MMLU (Ethics),Moral Reasoning,A subset of the MMLU benchmark that tests a model's ability to reason about moral scenarios.,https://arxiv.org/pdf/2009.03300,https://github.com/hendrycks/test
LTLBench,Temporal Logic Reasoning,"A benchmark with 2,000 challenges to evaluate the temporal logic reasoning capabilities of LLMs.",https://arxiv.org/pdf/2407.05434,https://github.com/RutaTang/LTLBench
AISafetyLab,Framework (Attack Defense Evaluation),A comprehensive and extensible framework from Tsinghua University for implementing and testing various AI safety mechanisms.,https://arxiv.org/pdf/2502.16776,https://github.com/thu-coai/AISafetyLab
IFEval,Instruction Following,"An evaluation suite for measuring how well large language models follow natural language instructions, including tests for prompt injection and jailbreaking.",https://arxiv.org/pdf/2311.07911,https://github.com/google-research/instruction-following-eval
Reefknot,Visual Reasoning and Safety,"A benchmark built from the Visual Genome scene graph dataset to evaluate perceptive and cognitive reasoning in vision-language models, with over 20,000 real-world samples.",https://arxiv.org/html/2408.09429v1,https://github.com/JackChen-seu/Reefknot